"""
LLMè§£ææ–‡æ¡£æµ‹è¯•ç´¢å¼•å™¨

ç‹¬ç«‹çš„ç´¢å¼•å™¨ï¼Œç”¨äºæµ‹è¯•LLMè§£æçš„Markdownæ–‡æ¡£
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

import uuid
from datetime import datetime
from typing import List, Dict, Any

from tests.llm_parsed_test.config import (
    LLM_PARSED_DOCS,
    TEST_PRODUCT,
    TEST_VECTOR_STORE_DIR,
    TEST_BM25_INDEX_PATH,
    CHUNK_SIZE,
    CHUNK_OVERLAP,
    CHROMA_COLLECTION_NAME,
    ensure_test_dirs
)
from tests.llm_parsed_test.chunking_strategy import create_table_aware_chunker
from src.common.models import PolicyChunk
from src.indexing.embedding.bge import BGEEmbedder
from src.indexing.vector_store.chroma import ChromaDBStore
from src.indexing.vector_store.hybrid_retriever import BM25Index
from src.common.logging import logger


class LLMTestIndexer:
    """LLMè§£ææ–‡æ¡£æµ‹è¯•ç´¢å¼•å™¨"""
    
    def __init__(self):
        ensure_test_dirs()
        
        self.chunker = create_table_aware_chunker(
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP
        )
        self.embedder = BGEEmbedder()
        self.chroma_store = ChromaDBStore(
            persist_directory=str(TEST_VECTOR_STORE_DIR)
        )
        self.bm25_index = BM25Index()
        
        logger.info(f"LLMTestIndexer initialized")
        logger.info(f"Vector store: {TEST_VECTOR_STORE_DIR}")
        logger.info(f"Collection: {CHROMA_COLLECTION_NAME}")
    
    def index_all_documents(self) -> Dict[str, Any]:
        """ç´¢å¼•æ‰€æœ‰LLMè§£æçš„æ–‡æ¡£"""
        logger.info("=" * 80)
        logger.info("å¼€å§‹ç´¢å¼•LLMè§£æçš„æ–‡æ¡£")
        logger.info("=" * 80)
        
        stats = {
            'total_documents': len(LLM_PARSED_DOCS),
            'total_chunks': 0,
            'success': 0,
            'failed': 0,
            'doc_stats': []
        }
        
        for doc_type, md_path in LLM_PARSED_DOCS.items():
            logger.info(f"\nå¤„ç†æ–‡æ¡£: {doc_type} ({md_path.name})")
            
            if not md_path.exists():
                logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {md_path}")
                stats['failed'] += 1
                continue
            
            try:
                # è¯»å–Markdown
                with open(md_path, 'r', encoding='utf-8') as f:
                    markdown_content = f.read()
                
                logger.info(f"æ–‡ä»¶å¤§å°: {len(markdown_content)} å­—ç¬¦")
                
                # ä½¿ç”¨LLMä¸“ç”¨chunkeråˆ†å—
                chunk_dicts = self.chunker.chunk_with_hierarchy(
                    markdown=markdown_content,
                    doc_id=f"llm-test-{doc_type}",
                    doc_type=doc_type
                )
                
                logger.info(f"ç”Ÿæˆäº† {len(chunk_dicts)} ä¸ªchunks")
                
                # è½¬æ¢ä¸ºPolicyChunkå¯¹è±¡
                chunks = self._create_policy_chunks(chunk_dicts, doc_type)
                
                # ç”Ÿæˆembeddings
                chunks = self._generate_embeddings(chunks)
                
                # ä¿å­˜åˆ°ChromaDB
                self.chroma_store.add_chunks(chunks)
                
                # æ›´æ–°BM25ç´¢å¼•
                for chunk in chunks:
                    self.bm25_index.add_chunk(chunk)
                
                stats['total_chunks'] += len(chunks)
                stats['success'] += 1
                stats['doc_stats'].append({
                    'doc_type': doc_type,
                    'chunks': len(chunks),
                    'file': md_path.name
                })
                
                logger.info(f"âœ… {doc_type}: {len(chunks)} chunks indexed")
                
            except Exception as e:
                logger.error(f"ç´¢å¼• {doc_type} å¤±è´¥: {e}")
                import traceback
                logger.error(traceback.format_exc())
                stats['failed'] += 1
        
        # ä¿å­˜BM25ç´¢å¼•
        try:
            self.bm25_index.save(str(TEST_BM25_INDEX_PATH))
            logger.info(f"\nBM25ç´¢å¼•å·²ä¿å­˜: {TEST_BM25_INDEX_PATH}")
        except Exception as e:
            logger.error(f"ä¿å­˜BM25ç´¢å¼•å¤±è´¥: {e}")
        
        # æ‰“å°æ€»ç»“
        logger.info("\n" + "=" * 80)
        logger.info("ç´¢å¼•å®Œæˆ!")
        logger.info("=" * 80)
        logger.info(f"æˆåŠŸæ–‡æ¡£: {stats['success']}/{stats['total_documents']}")
        logger.info(f"æ€»chunks: {stats['total_chunks']}")
        logger.info(f"å¤±è´¥: {stats['failed']}")
        
        for doc_stat in stats['doc_stats']:
            logger.info(f"  - {doc_stat['doc_type']}: {doc_stat['chunks']} chunks")
        
        logger.info("=" * 80)
        
        return stats
    
    def _create_policy_chunks(self, chunk_dicts: List[Dict], doc_type: str) -> List[PolicyChunk]:
        """å°†chunkå­—å…¸è½¬ä¸ºPolicyChunkå¯¹è±¡"""
        chunks = []
        
        for chunk_dict in chunk_dicts:
            chunk = PolicyChunk(
                id=str(uuid.uuid4()),
                document_id=f"llm-test-{doc_type}",
                company=TEST_PRODUCT['company'],
                product_code=TEST_PRODUCT['product_code'],
                product_name=TEST_PRODUCT['name'],
                doc_type=doc_type,
                content=chunk_dict['content'],
                section_id=chunk_dict.get('section_id', ''),
                section_title=chunk_dict.get('section_title', ''),
                section_path=chunk_dict.get('section_path', ''),
                level=chunk_dict.get('level', 1),
                chunk_index=chunk_dict.get('chunk_index', 0),
                category=chunk_dict.get('category', 'General'),
                table_refs=[],
                entity_role='Insured',  # é»˜è®¤å€¼ï¼Œç¬¦åˆæšä¸¾è¦æ±‚
                keywords=[],  # å¿…é¡»æ˜¯åˆ—è¡¨
                created_at=datetime.now()
            )
            chunks.append(chunk)
        
        return chunks
    
    def _generate_embeddings(self, chunks: List[PolicyChunk]) -> List[PolicyChunk]:
        """æ‰¹é‡ç”Ÿæˆembeddings"""
        logger.info("ç”Ÿæˆembeddings...")
        
        contents = [chunk.content for chunk in chunks]
        embeddings = self.embedder.embed_batch(contents)
        
        for chunk, embedding in zip(chunks, embeddings):
            chunk.embedding_vector = embedding
        
        stats = self.embedder.get_stats()
        logger.info(f"Embeddingsç”Ÿæˆå®Œæˆ: {stats['total_tokens']} tokens")
        
        return chunks
    
    def reset(self):
        """é‡ç½®æµ‹è¯•ç¯å¢ƒ"""
        logger.warning("é‡ç½®æµ‹è¯•ç¯å¢ƒ...")
        self.chroma_store.reset()
        self.bm25_index = BM25Index()
        logger.info("æµ‹è¯•ç¯å¢ƒå·²é‡ç½®")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    from src.common.logging import setup_logging
    
    parser = argparse.ArgumentParser(description="LLMè§£ææ–‡æ¡£æµ‹è¯•ç´¢å¼•å™¨")
    parser.add_argument('--rebuild', action='store_true', help="é‡å»ºç´¢å¼•ï¼ˆæ¸…ç©ºç°æœ‰æ•°æ®ï¼‰")
    
    args = parser.parse_args()
    
    setup_logging()
    
    indexer = LLMTestIndexer()
    
    if args.rebuild:
        indexer.reset()
    
    stats = indexer.index_all_documents()
    
    # æ‰“å°ChromaDBç»Ÿè®¡
    chroma_stats = indexer.chroma_store.get_stats()
    print(f"\nğŸ“Š ChromaDBç»Ÿè®¡:")
    print(f"  - Collection: {chroma_stats['collection_name']}")
    print(f"  - Total chunks: {chroma_stats['total_chunks']}")
    print(f"  - Vector dimension: {chroma_stats['vector_dimension']}")
    print(f"  - Distance metric: {chroma_stats['distance_metric']}")
    
    if stats['success'] == stats['total_documents']:
        print(f"\nâœ… æ‰€æœ‰æ–‡æ¡£ç´¢å¼•æˆåŠŸ!")
    else:
        print(f"\nâš ï¸ {stats['failed']} ä¸ªæ–‡æ¡£ç´¢å¼•å¤±è´¥")


if __name__ == "__main__":
    main()
