"""
LLMè§£ææ–‡æ¡£ç«¯åˆ°ç«¯æµ‹è¯•æ‰§è¡Œå™¨

è¿è¡Œ20ä¸ªæµ‹è¯•é—®é¢˜ï¼Œè°ƒç”¨MCPæœåŠ¡ï¼Œç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

import json
from datetime import datetime
from typing import List, Dict, Any

from tests.llm_parsed_test.config import (
    TEST_QUESTIONS_PATH,
    TEST_REPORT_PATH,
    TEST_VECTOR_STORE_DIR,
    TEST_BM25_INDEX_PATH,
    CHROMA_COLLECTION_NAME
)
from src.indexing.embedding.bge import BGEEmbedder
from src.indexing.vector_store.chroma import ChromaDBStore
from src.indexing.vector_store.hybrid_retriever import BM25Index, create_hybrid_retriever
from src.common.logging import logger


class LLMTestRunner:
    """LLMè§£ææ–‡æ¡£ç«¯åˆ°ç«¯æµ‹è¯•æ‰§è¡Œå™¨"""
    
    def __init__(self):
        # åˆå§‹åŒ–æ£€ç´¢ç»„ä»¶ï¼ˆæŒ‡å‘æµ‹è¯•ç¯å¢ƒï¼‰
        self.embedder = BGEEmbedder()
        self.chroma_store = ChromaDBStore(
            persist_directory=str(TEST_VECTOR_STORE_DIR)
        )
        
        # åŠ è½½BM25ç´¢å¼•
        try:
            self.bm25_index = BM25Index()
            self.bm25_index.load(str(TEST_BM25_INDEX_PATH))
            self.hybrid_retriever = create_hybrid_retriever(
                chroma_store=self.chroma_store,
                bm25_index=self.bm25_index
            )
            logger.info("BM25ç´¢å¼•åŠ è½½æˆåŠŸï¼Œå°†ä½¿ç”¨æ··åˆæ£€ç´¢")
        except FileNotFoundError:
            logger.warning("BM25ç´¢å¼•æœªæ‰¾åˆ°ï¼Œå°†ä½¿ç”¨çº¯å‘é‡æ£€ç´¢")
            self.bm25_index = None
            self.hybrid_retriever = None
        
        logger.info(f"TestRunner initialized")
    
    def load_questions(self) -> List[Dict]:
        """åŠ è½½æµ‹è¯•é—®é¢˜"""
        with open(TEST_QUESTIONS_PATH, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        questions = data['questions']
        logger.info(f"åŠ è½½äº† {len(questions)} ä¸ªæµ‹è¯•é—®é¢˜")
        return questions
    
    def run_single_query(self, question: str, top_k: int = 3) -> List[Dict]:
        """
        æ‰§è¡Œå•ä¸ªæŸ¥è¯¢
        
        Returns:
            List of results with similarity scores and metadata
        """
        # ç”Ÿæˆquery embedding
        query_embedding = self.embedder.embed_single(question)
        
        # æ‰§è¡Œæ£€ç´¢
        if self.hybrid_retriever:
            # æ··åˆæ£€ç´¢
            results = self.hybrid_retriever.search(
                query=question,
                query_embedding=query_embedding,
                n_results=top_k,
                auto_weight=True
            )
        else:
            # çº¯å‘é‡æ£€ç´¢
            results = self.chroma_store.search(
                query_embedding=query_embedding,
                n_results=top_k
            )
        
        return results
    
    def evaluate_result(self, results: List[Dict], expected_answer: Dict) -> Dict[str, Any]:
        """
        è¯„ä¼°æ£€ç´¢ç»“æœæ˜¯å¦åŒ…å«æ­£ç¡®ç­”æ¡ˆ
        
        Returns:
            evaluation dict with hit@1, hit@3, scores
        """
        evaluation = {
            'hit_at_1': False,
            'hit_at_3': False,
            'top1_score': 0.0,
            'top3_scores': [],
            'reasoning': ''
        }
        
        if not results:
            evaluation['reasoning'] = "æ— æ£€ç´¢ç»“æœ"
            return evaluation
        
        # æå–é¢„æœŸçš„å…³é”®ä¿¡æ¯
        expected_key_phrase = expected_answer.get('key_phrase', '')
        expected_section_id = expected_answer.get('section_id', '')
        expected_doc_type = expected_answer.get('doc_type', '')
        
        # è¯„ä¼°Top-1
        top1_result = results[0]
        top1_metadata = top1_result.get('metadata', {})
        top1_content = top1_result.get('document', '')
        
        # å¤„ç†distanceæˆ–rrf_score
        distance = top1_result.get('distance')
        if distance is not None:
            evaluation['top1_score'] = 1 - distance
        else:
            # æ··åˆæ£€ç´¢è¿”å›rrf_score
            evaluation['top1_score'] = top1_result.get('rrf_score', 0.0)
        
        # æ£€æŸ¥Top-1æ˜¯å¦åŒ¹é…
        if self._is_match(top1_metadata, top1_content, expected_section_id, expected_key_phrase, expected_doc_type):
            evaluation['hit_at_1'] = True
            evaluation['hit_at_3'] = True
            evaluation['reasoning'] = f"Top-1å‘½ä¸­: ç›¸ä¼¼åº¦{evaluation['top1_score']:.3f}"
        else:
            # æ£€æŸ¥Top-3
            for i, result in enumerate(results[:3], 1):
                metadata = result.get('metadata', {})
                content = result.get('document', '')
                
                # å¤„ç†distanceæˆ–rrf_score
                distance = result.get('distance')
                if distance is not None:
                    score = 1 - distance
                else:
                    score = result.get('rrf_score', 0.0)
                
                evaluation['top3_scores'].append(score)
                
                if self._is_match(metadata, content, expected_section_id, expected_key_phrase, expected_doc_type):
                    evaluation['hit_at_3'] = True
                    evaluation['reasoning'] = f"Top-{i}å‘½ä¸­: ç›¸ä¼¼åº¦{score:.3f}"
                    break
            
            if not evaluation['hit_at_3']:
                evaluation['reasoning'] = f"Top-3æœªå‘½ä¸­ï¼ˆæœ€é«˜ç›¸ä¼¼åº¦: {evaluation['top1_score']:.3f}ï¼‰"
        
        return evaluation
    
    def _is_match(self, metadata: Dict, content: str, expected_section_id: str, 
                  expected_key_phrase: str, expected_doc_type: str) -> bool:
        """åˆ¤æ–­ç»“æœæ˜¯å¦åŒ¹é…é¢„æœŸç­”æ¡ˆ"""
        # æ£€æŸ¥æ–‡æ¡£ç±»å‹
        if expected_doc_type and metadata.get('doc_type') != expected_doc_type:
            # å¦‚æœé¢„æœŸå¤šä¸ªdoc_typeï¼ˆç”¨é€—å·åˆ†éš”ï¼‰
            if ',' in expected_doc_type:
                doc_types = [dt.strip() for dt in expected_doc_type.split(',')]
                if metadata.get('doc_type') not in doc_types:
                    return False
            else:
                return False
        
        # æ£€æŸ¥section_idï¼ˆå¦‚æœæä¾›ï¼‰
        if expected_section_id:
            # å¯èƒ½æœ‰å¤šä¸ªsection_idï¼ˆé€—å·åˆ†éš”ï¼‰
            if ',' in expected_section_id:
                section_ids = [sid.strip() for sid in expected_section_id.split(',')]
                if metadata.get('section_id') not in section_ids:
                    return False
            else:
                if metadata.get('section_id') != expected_section_id:
                    return False
        
        # æ£€æŸ¥å…³é”®çŸ­è¯­
        if expected_key_phrase and expected_key_phrase in content:
            return True
        
        # å®½æ¾åŒ¹é…ï¼šsection_idæ­£ç¡®å³å¯
        if expected_section_id and metadata.get('section_id') == expected_section_id:
            return True
        
        return False
    
    def run_all_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•é—®é¢˜"""
        logger.info("=" * 80)
        logger.info("å¼€å§‹æ‰§è¡Œæµ‹è¯•")
        logger.info("=" * 80)
        
        questions = self.load_questions()
        
        results = {
            'metadata': {
                'total_questions': len(questions),
                'test_time': datetime.now().isoformat(),
                'retrieval_method': 'hybrid' if self.hybrid_retriever else 'semantic'
            },
            'category_stats': {},
            'question_results': []
        }
        
        hit_at_1_count = 0
        hit_at_3_count = 0
        
        for i, question_item in enumerate(questions, 1):
            question = question_item['question']
            category = question_item['category']
            expected_answer = question_item['expected_answer']
            
            logger.info(f"\n[{i}/{len(questions)}] {question}")
            
            # æ‰§è¡ŒæŸ¥è¯¢
            search_results = self.run_single_query(question, top_k=3)
            
            # è¯„ä¼°ç»“æœ
            evaluation = self.evaluate_result(search_results, expected_answer)
            
            # è®°å½•ç»“æœ
            question_result = {
                'id': question_item['id'],
                'question': question,
                'category': category,
                'expected_answer': expected_answer,
                'search_results': search_results,
                'evaluation': evaluation
            }
            results['question_results'].append(question_result)
            
            # æ›´æ–°ç»Ÿè®¡
            if evaluation['hit_at_1']:
                hit_at_1_count += 1
            if evaluation['hit_at_3']:
                hit_at_3_count += 1
            
            # æ‰“å°è¯„ä¼°ç»“æœ
            status = "âœ…" if evaluation['hit_at_1'] else ("ğŸ”¶" if evaluation['hit_at_3'] else "âŒ")
            logger.info(f"  {status} {evaluation['reasoning']}")
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        results['overall_stats'] = {
            'hit_at_1_count': hit_at_1_count,
            'hit_at_3_count': hit_at_3_count,
            'hit_at_1_rate': hit_at_1_count / len(questions),
            'hit_at_3_rate': hit_at_3_count / len(questions)
        }
        
        # æŒ‰ç±»åˆ«ç»Ÿè®¡
        for category in set(q['category'] for q in questions):
            category_questions = [r for r in results['question_results'] if r['category'] == category]
            category_hit_1 = sum(1 for r in category_questions if r['evaluation']['hit_at_1'])
            category_hit_3 = sum(1 for r in category_questions if r['evaluation']['hit_at_3'])
            
            results['category_stats'][category] = {
                'total': len(category_questions),
                'hit_at_1': category_hit_1,
                'hit_at_3': category_hit_3,
                'hit_at_1_rate': category_hit_1 / len(category_questions) if category_questions else 0,
                'hit_at_3_rate': category_hit_3 / len(category_questions) if category_questions else 0
            }
        
        logger.info("\n" + "=" * 80)
        logger.info("æµ‹è¯•å®Œæˆ!")
        logger.info("=" * 80)
        logger.info(f"Hit@1: {hit_at_1_count}/{len(questions)} ({results['overall_stats']['hit_at_1_rate']:.1%})")
        logger.info(f"Hit@3: {hit_at_3_count}/{len(questions)} ({results['overall_stats']['hit_at_3_rate']:.1%})")
        
        return results
    
    def generate_report(self, test_results: Dict[str, Any]):
        """ç”ŸæˆMarkdownæµ‹è¯•æŠ¥å‘Š"""
        logger.info(f"\nç”Ÿæˆæµ‹è¯•æŠ¥å‘Š: {TEST_REPORT_PATH}")
        
        TEST_REPORT_PATH.parent.mkdir(parents=True, exist_ok=True)
        
        lines = []
        
        # æ ‡é¢˜
        lines.extend([
            f"# LLMè§£ææ–‡æ¡£æµ‹è¯•æŠ¥å‘Š",
            f"",
            f"**æµ‹è¯•æ—¶é—´**: {test_results['metadata']['test_time']}",
            f"**æ£€ç´¢æ–¹æ³•**: {test_results['metadata']['retrieval_method']}",
            f"**é—®é¢˜æ€»æ•°**: {test_results['metadata']['total_questions']}",
            f"",
            f"---",
            f""
        ])
        
        # æ‰§è¡Œæ‘˜è¦
        overall = test_results['overall_stats']
        lines.extend([
            f"## ğŸ“Š æ‰§è¡Œæ‘˜è¦",
            f"",
            f"| æŒ‡æ ‡ | æ•°å€¼ |",
            f"|------|------|",
            f"| Hit@1å‡†ç¡®ç‡ | {overall['hit_at_1_count']}/{test_results['metadata']['total_questions']} ({overall['hit_at_1_rate']:.1%}) |",
            f"| Hit@3å‡†ç¡®ç‡ | {overall['hit_at_3_count']}/{test_results['metadata']['total_questions']} ({overall['hit_at_3_rate']:.1%}) |",
            f"",
        ])
        
        # åˆ†ç±»ç»Ÿè®¡
        lines.extend([
            f"## ğŸ“ˆ åˆ†ç±»ç»Ÿè®¡",
            f"",
            f"| ç±»åˆ« | æ€»æ•° | Hit@1 | Hit@3 |",
            f"|------|------|-------|-------|",
        ])
        
        category_names = {
            'basic': 'åŸºç¡€æŸ¥è¯¢',
            'comparison': 'å¯¹æ¯”æŸ¥è¯¢',
            'rate': 'è´¹ç‡æŸ¥è¯¢',
            'exclusion': 'å…è´£æŸ¥è¯¢'
        }
        
        for category, stats in test_results['category_stats'].items():
            category_cn = category_names.get(category, category)
            lines.append(
                f"| {category_cn} | {stats['total']} | "
                f"{stats['hit_at_1']}/{stats['total']} ({stats['hit_at_1_rate']:.1%}) | "
                f"{stats['hit_at_3']}/{stats['total']} ({stats['hit_at_3_rate']:.1%}) |"
            )
        
        lines.append("")
        
        # è¯¦ç»†ç»“æœ
        lines.extend([
            f"## ğŸ“ è¯¦ç»†ç»“æœ",
            f""
        ])
        
        for result in test_results['question_results']:
            q_id = result['id']
            question = result['question']
            category = category_names.get(result['category'], result['category'])
            evaluation = result['evaluation']
            
            # é—®é¢˜æ ‡é¢˜
            status_emoji = "âœ…" if evaluation['hit_at_1'] else ("ğŸ”¶" if evaluation['hit_at_3'] else "âŒ")
            lines.extend([
                f"### {status_emoji} é—®é¢˜ #{q_id}: {question}",
                f"",
                f"**ç±»åˆ«**: {category}  ",
                f"**è¯„ä¼°**: {evaluation['reasoning']}",
                f"",
            ])
            
            # Top-3ç»“æœ
            lines.append(f"**æ£€ç´¢ç»“æœ**:")
            lines.append(f"")
            
            for i, search_result in enumerate(result['search_results'][:3], 1):
                metadata = search_result.get('metadata', {})
                content = search_result.get('document', '')
                
                # å¤„ç†distanceæˆ–rrf_score  
                distance = search_result.get('distance')
                if distance is not None:
                    score = 1 - distance
                else:
                    score = search_result.get('rrf_score', 0.0)
                
                lines.extend([
                    f"{i}. **ç›¸ä¼¼åº¦**: {score:.3f} | **æ–‡æ¡£**: {metadata.get('doc_type', 'N/A')} | "
                    f"**ç« èŠ‚**: {metadata.get('section_title', 'N/A')} ({metadata.get('section_id', '')})",
                    f"   ```",
                    f"   {content[:200]}...",
                    f"   ```",
                    f""
                ])
            
            lines.append("---")
            lines.append("")
        
        # å†™å…¥æ–‡ä»¶
        with open(TEST_REPORT_PATH, 'w', encoding='utf-8') as f:
            f.write('\n'.join(lines))
        
        logger.info(f"âœ… æµ‹è¯•æŠ¥å‘Šå·²ç”Ÿæˆ: {TEST_REPORT_PATH}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    from src.common.logging import setup_logging
    
    parser = argparse.ArgumentParser(description="LLMè§£ææ–‡æ¡£ç«¯åˆ°ç«¯æµ‹è¯•")
    parser.add_argument('--output', default=str(TEST_REPORT_PATH), help="æµ‹è¯•æŠ¥å‘Šè¾“å‡ºè·¯å¾„")
    
    args = parser.parse_args()
    
    setup_logging()
    
    runner = LLMTestRunner()
    test_results = runner.run_all_tests()
    runner.generate_report(test_results)
    
    # æ‰“å°æ€»ç»“
    overall = test_results['overall_stats']
    print(f"\n" + "=" * 80)
    print(f"æµ‹è¯•å®Œæˆ! æŠ¥å‘Šå·²ä¿å­˜: {TEST_REPORT_PATH}")
    print(f"=" * 80)
    print(f"Hit@1å‡†ç¡®ç‡: {overall['hit_at_1_rate']:.1%} ({overall['hit_at_1_count']}/{test_results['metadata']['total_questions']})")
    print(f"Hit@3å‡†ç¡®ç‡: {overall['hit_at_3_rate']:.1%} ({overall['hit_at_3_count']}/{test_results['metadata']['total_questions']})")
    print(f"=" * 80)
    
    # å»ºè®®
    if overall['hit_at_1_rate'] >= 0.75:
        print(f"âœ… è¾¾åˆ°ç›®æ ‡æ ‡å‡† (Hit@1 â‰¥ 75%)")
    elif overall['hit_at_3_rate'] >= 0.70:
        print(f"ğŸ”¶ è¾¾åˆ°æœ€ä½æ ‡å‡† (Hit@3 â‰¥ 70%)")
    else:
        print(f"âŒ æœªè¾¾åˆ°æœ€ä½æ ‡å‡†ï¼Œå»ºè®®ä¼˜åŒ–chunkingç­–ç•¥æˆ–embeddingæ¨¡å‹")


if __name__ == "__main__":
    main()
