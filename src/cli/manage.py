import typer
import asyncio
import json
from typing import Optional
from src.common.db import init_db
from src.common.config import config
from src.common.logging import setup_logging
from src.crawler.discovery.iac_spider import IACSpider
from src.crawler.pipelines.save_pipeline import save_pipeline

app = typer.Typer(help="Insurance MCP Management CLI")
crawl_app = typer.Typer(help="Crawler commands")
process_app = typer.Typer(help="PDF processing commands")
index_app = typer.Typer(help="Indexing commands")
app.add_typer(crawl_app, name="crawl")
app.add_typer(process_app, name="process")
app.add_typer(index_app, name="index")

@app.callback()
def callback():
    """
    Insurance MCP Management CLI
    """
    pass

@app.command()
def init():
    """Initialize the application: database and directories."""
    setup_logging()
    typer.echo("Initializing Insurance MCP...")
    config.ensure_dirs()
    init_db()
    typer.echo("Initialization complete.")

@crawl_app.command()
def discover(company: Optional[str] = None, limit: int = 10, output: Optional[str] = None, headless: bool = True):
    """Discover products from IAC."""
    setup_logging()
    spider = IACSpider(headless=headless)
    products = asyncio.run(spider.discover_products(company_filter=company, limit=limit))
    
    if output:
        with open(output, 'w') as f:
            json.dump(products, f, ensure_ascii=False, indent=2)
        typer.echo(f"Saved {len(products)} products to {output}")
    else:
        typer.echo(json.dumps(products, ensure_ascii=False, indent=2))

@crawl_app.command()
def acquire(input_file: str):
    """Download PDF documents from a JSON file (output of discover)."""
    setup_logging()
    
    try:
        with open(input_file, 'r') as f:
            items = json.load(f)
    except Exception as e:
        typer.echo(f"Error reading input file: {e}")
        raise typer.Exit(code=1)
    
    async def run_pipeline():
        for item in items:
            await save_pipeline.process_item(item)
            
    asyncio.run(run_pipeline())
    typer.echo("Acquisition complete.")

@crawl_app.command()
def run(
    company: str = typer.Option("pingan-life", help="å…¬å¸ä»£ç  (pingan-life)"),
    limit: int = typer.Option(100, help="æœ€å¤§çˆ¬å–äº§å“æ•°é‡")
):
    """
    è¿è¡Œå®Œæ•´çš„é‡‡é›†æµç¨‹: å‘ç°äº§å“ -> ä¸‹è½½PDF -> ä¿å­˜åˆ°æ•°æ®åº“
    
    è¿™æ˜¯ä¸€ç«™å¼å‘½ä»¤ï¼ŒåŒ…å«äº†discoverå’Œacquireçš„æ‰€æœ‰åŠŸèƒ½ã€‚
    """
    setup_logging()
    
    try:
        # å…¬å¸ä»£ç æ˜ å°„
        company_map = {
            "pingan-life": "å¹³å®‰äººå¯¿",
        }
        
        company_name = company_map.get(company)
        if not company_name:
            typer.echo(f"âŒ ä¸æ”¯æŒçš„å…¬å¸: {company}. æ”¯æŒ: {', '.join(company_map.keys())}")
            raise typer.Exit(code=1)
        
        typer.echo(f"ğŸš€ å¼€å§‹é‡‡é›† {company_name} æ•°æ®...")
        typer.echo(f"é…ç½®: limit={limit}\n")
        
        # å¯¼å…¥å¹¶è¿è¡Œé‡‡é›†ç®¡é“
        from src.crawler.pipelines.acquisition_pipeline import run_acquisition
        
        stats = asyncio.run(run_acquisition(company=company_name, limit=limit))
        
        typer.echo(f"\n" + "="*60)
        typer.echo(f"âœ… é‡‡é›†å®Œæˆ!")
        typer.echo(f"="*60)
        typer.echo(f"äº§å“: å‘ç° {stats['products_discovered']}, æ–°å¢ {stats['products_new']}, å·²å­˜åœ¨ {stats['products_existing']}")
        typer.echo(f"PDF: ä¸‹è½½ {stats['pdfs_downloaded']}, è·³è¿‡ {stats['pdfs_skipped']}, å¤±è´¥ {stats['pdfs_failed']}")
        typer.echo(f"="*60)
        
        if stats['pdfs_failed'] > 0:
            typer.echo(f"âš ï¸  æœ‰ {stats['pdfs_failed']} ä¸ªPDFä¸‹è½½å¤±è´¥ï¼Œè¯·æŸ¥çœ‹æ—¥å¿—")
        
    except KeyboardInterrupt:
        typer.echo("\nâš ï¸  é‡‡é›†å·²ä¸­æ–­")
        raise typer.Exit(code=130)
    except Exception as e:
        typer.echo(f"âŒ é‡‡é›†å¤±è´¥: {e}")
        import traceback
        from src.common.logging import logger
        logger.error(traceback.format_exc())
        raise typer.Exit(code=1)

@process_app.command("convert")
def process_convert(
    doc_type: str = typer.Option(None, help="æ–‡æ¡£ç±»å‹è¿‡æ»¤ (äº§å“æ¡æ¬¾/äº§å“è¯´æ˜ä¹¦/äº§å“è´¹ç‡è¡¨)"),
    limit: int = typer.Option(10, help="æœ€å¤šè½¬æ¢æ–‡æ¡£æ•°"),
    all_docs: bool = typer.Option(False, "--all", help="è½¬æ¢æ‰€æœ‰PENDINGæ–‡æ¡£")
):
    """
    å°†PENDINGçŠ¶æ€çš„PDFæ–‡æ¡£è½¬æ¢ä¸ºMarkdown
    
    æ”¯æŒçš„æ–‡æ¡£ç±»å‹ï¼š
    - äº§å“æ¡æ¬¾
    - äº§å“è¯´æ˜ä¹¦
    - äº§å“è´¹ç‡è¡¨
    
    ç¤ºä¾‹ï¼š
    - python -m src.cli.manage process convert --doc-type äº§å“æ¡æ¬¾ --limit 5
    - python -m src.cli.manage process convert --all
    """
    setup_logging()
    
    from src.parser.markdown.converter import get_converter
    
    typer.echo("\nğŸ”„ å¼€å§‹PDFè½¬Markdownè½¬æ¢...")
    typer.echo(f"æ–‡æ¡£ç±»å‹è¿‡æ»¤: {doc_type or 'å…¨éƒ¨ï¼ˆæ¡æ¬¾+è¯´æ˜ä¹¦ï¼‰'}")
    typer.echo(f"é™åˆ¶æ•°é‡: {'æ— é™åˆ¶' if all_docs else limit}\n")
    
    converter = get_converter()
    
    # æ‰§è¡Œè½¬æ¢
    stats = converter.convert_batch(
        doc_type_filter=doc_type,
        limit=999999 if all_docs else limit
    )
    
    # æ˜¾ç¤ºç»“æœ
    typer.echo("\n" + "="*60)
    typer.echo("âœ… è½¬æ¢å®Œæˆ")
    typer.echo("="*60)
    typer.echo(f"æ€»è®¡: {stats['total']}")
    typer.echo(f"æˆåŠŸ: {stats['success']}")
    typer.echo(f"å¤±è´¥: {stats['failed']}")
    typer.echo("="*60)
    
    if stats['failed'] > 0:
        typer.echo("\nâš ï¸  éƒ¨åˆ†æ–‡æ¡£è½¬æ¢å¤±è´¥ï¼Œè¯·æŸ¥çœ‹æ—¥å¿—")
    
    if stats['success'] > 0:
        typer.echo(f"\nğŸ’¡ æç¤º: è½¬æ¢åçš„Markdownæ–‡ä»¶ä¿å­˜åœ¨ data/processed/ ç›®å½•")
        typer.echo(f"   ä½¿ç”¨ 'python -m src.cli.verify' å‘½ä»¤è¿›è¡Œäººå·¥å®¡æ ¸")

@process_app.command("analyze")
def process_analyze(
    product_code: str = typer.Argument(..., help="äº§å“ä»£ç ï¼Œå¦‚ 5004")
):
    """
    åˆ†ææŒ‡å®šäº§å“çš„PDFæ–‡æ¡£ç‰ˆé¢ç»“æ„
    
    ç¤ºä¾‹:
    - python -m src.cli.manage process analyze 5004
    """
    setup_logging()
    
    from src.common.repository import SQLiteRepository
    from src.parser.layout.analyzer import get_analyzer
    from pathlib import Path
    
    typer.echo(f"\nğŸ” åˆ†æäº§å“ {product_code} çš„PDFæ–‡æ¡£ç‰ˆé¢...\n")
    
    repo = SQLiteRepository()
    analyzer = get_analyzer()
    
    # è·å–è¯¥äº§å“çš„æ‰€æœ‰æ–‡æ¡£
    with repo.get_db_connection() as conn:
        cursor = conn.cursor()
        rows = cursor.execute(
            """
            SELECT id, doc_type, filename, local_path 
            FROM policy_documents 
            WHERE product_id = (SELECT id FROM products WHERE product_code = ?)
            AND doc_type IN ('äº§å“æ¡æ¬¾', 'äº§å“è¯´æ˜ä¹¦')
            """,
            (product_code,)
        ).fetchall()
    
    if not rows:
        typer.echo(f"âŒ æœªæ‰¾åˆ°äº§å“ {product_code} çš„æ¡æ¬¾æˆ–è¯´æ˜ä¹¦æ–‡æ¡£")
        raise typer.Exit(code=1)
    
    # åˆ†ææ¯ä¸ªæ–‡æ¡£
    for row in rows:
        doc_type = row[1]
        filename = row[2]
        local_path = row[3]
        
        typer.echo(f"ğŸ“„ {doc_type}: {filename}")
        
        pdf_path = Path(local_path)
        if not pdf_path.exists():
            typer.echo(f"   âŒ æ–‡ä»¶ä¸å­˜åœ¨: {local_path}\n")
            continue
        
        result = analyzer.analyze_pdf(pdf_path)
        
        if result["success"]:
            typer.echo(f"   âœ… åˆ†ææˆåŠŸ")
            typer.echo(f"   é¡µæ•°: {result['total_pages']}")
            typer.echo(f"   å¸ƒå±€ç±»å‹: {result['layout_type']}")
            typer.echo(f"   åŒ…å«è¡¨æ ¼: {'æ˜¯' if result['has_tables'] else 'å¦'}")
            typer.echo(f"   åŒ…å«å›¾åƒ: {'æ˜¯' if result['has_images'] else 'å¦'}")
            
            quality = analyzer.get_quality_score(result)
            typer.echo(f"   è´¨é‡è¯„åˆ†: {quality:.2f}")
            
            if quality < 0.8:
                typer.echo(f"   âš ï¸  å»ºè®®äººå·¥å¤æ ¸")
        else:
            typer.echo(f"   âŒ åˆ†æå¤±è´¥: {result['error']}")
        
        typer.echo("")


@process_app.command("postprocess")
def process_postprocess(
    doc_id: Optional[str] = typer.Option(None, "--doc-id", help="æŒ‡å®šæ–‡æ¡£IDè¿›è¡Œåå¤„ç†"),
    all_docs: bool = typer.Option(False, "--all", help="åå¤„ç†æ‰€æœ‰VERIFIEDæ–‡æ¡£"),
    steps: Optional[str] = typer.Option(None, "--steps", help="æŒ‡å®šæ‰§è¡Œçš„æ­¥éª¤ï¼ˆé€—å·åˆ†éš”ï¼‰ï¼Œå¦‚: footnote,noise,format"),
):
    """
    å¯¹å·²è½¬æ¢çš„Markdownæ–‡æ¡£æ‰§è¡Œåå¤„ç†
    
    åå¤„ç†æ­¥éª¤åŒ…æ‹¬ï¼š
    1. footnote: è„šæ³¨å†…è”ï¼ˆæå‡50%æ£€ç´¢æ•ˆæœï¼‰
    2. noise: å™ªéŸ³å»é™¤ï¼ˆé¡µçœ‰ã€é¡µè„šã€æ°´å°ï¼‰
    3. format: æ ¼å¼æ ‡å‡†åŒ–ï¼ˆç»Ÿä¸€æ ‡é¢˜ã€åˆ—è¡¨æ ¼å¼ï¼‰
    4. table: è¡¨æ ¼éªŒè¯ï¼ˆæ£€æŸ¥è¡Œåˆ—å®Œæ•´æ€§ï¼‰
    
    ç¤ºä¾‹:
    - python -m src.cli.manage process postprocess --all
    - python -m src.cli.manage process postprocess --doc-id doc123
    - python -m src.cli.manage process postprocess --all --steps footnote,noise
    """
    setup_logging()
    
    from src.common.repository import SQLiteRepository
    from src.parser.markdown.postprocessor import MarkdownPostProcessor
    from pathlib import Path
    
    if not doc_id and not all_docs:
        typer.echo("âŒ è¯·æŒ‡å®š --doc-id æˆ– --all å‚æ•°")
        raise typer.Exit(code=1)
    
    # è§£ææ­¥éª¤å‚æ•°
    step_list = None
    if steps:
        step_list = [s.strip() for s in steps.split(',')]
        typer.echo(f"ğŸ“ æ‰§è¡Œæ­¥éª¤: {', '.join(step_list)}\n")
    
    # åˆå§‹åŒ–åå¤„ç†å™¨
    processor = MarkdownPostProcessor(steps=step_list)
    
    repo = SQLiteRepository()
    
    # è·å–è¦å¤„ç†çš„æ–‡æ¡£
    with repo.get_db_connection() as conn:
        cursor = conn.cursor()
        
        if doc_id:
            # å¤„ç†å•ä¸ªæ–‡æ¡£
            rows = cursor.execute(
                """
                SELECT id, filename, local_path 
                FROM policy_documents 
                WHERE id = ? AND verification_status = 'VERIFIED'
                """,
                (doc_id,)
            ).fetchall()
        else:
            # å¤„ç†æ‰€æœ‰VERIFIEDæ–‡æ¡£
            rows = cursor.execute(
                """
                SELECT id, filename, local_path 
                FROM policy_documents 
                WHERE verification_status = 'VERIFIED'
                AND markdown_content IS NOT NULL
                """
            ).fetchall()
    
    if not rows:
        typer.echo("âŒ æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ–‡æ¡£")
        raise typer.Exit(code=1)
    
    typer.echo(f"ğŸ”§ æ‰¾åˆ° {len(rows)} ä¸ªæ–‡æ¡£éœ€è¦åå¤„ç†\n")
    
    success_count = 0
    fail_count = 0
    
    # å¤„ç†æ¯ä¸ªæ–‡æ¡£
    for row in rows:
        doc_id_val = row[0]
        filename = row[1]
        
        typer.echo(f"ğŸ“„ å¤„ç†: {filename} (ID: {doc_id_val[:8]}...)")
        
        # è·å–Markdownæ–‡ä»¶è·¯å¾„
        md_path = Path(f"data/processed/{doc_id_val}.md")
        
        if not md_path.exists():
            typer.echo(f"   âš ï¸  Markdownæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡\n")
            fail_count += 1
            continue
        
        try:
            # æ‰§è¡Œåå¤„ç†
            processor.process(str(md_path))
            typer.echo(f"   âœ… åå¤„ç†å®Œæˆ\n")
            success_count += 1
        except Exception as e:
            typer.echo(f"   âŒ åå¤„ç†å¤±è´¥: {e}\n")
            fail_count += 1
    
    # æ€»ç»“
    typer.echo(f"\n{'='*60}")
    typer.echo(f"âœ… æˆåŠŸ: {success_count}")
    typer.echo(f"âŒ å¤±è´¥: {fail_count}")
    typer.echo(f"{'='*60}\n")


# ============================================================================
# Index Commands
# ============================================================================

@index_app.command("rebuild")
def index_rebuild(
    reset: bool = typer.Option(False, "--reset", help="å…ˆæ¸…ç©ºç°æœ‰ç´¢å¼•"),
    enable_bm25: bool = typer.Option(True, "--enable-bm25/--no-bm25", help="æ˜¯å¦æ„å»ºBM25ç´¢å¼•"),
):
    """
    é‡å»ºå‘é‡ç´¢å¼•
    
    ä»æ‰€æœ‰VERIFIEDæ–‡æ¡£é‡æ–°æ„å»ºChromaDBå’ŒBM25ç´¢å¼•ã€‚
    
    ç¤ºä¾‹:
    - python -m src.cli.manage index rebuild
    - python -m src.cli.manage index rebuild --reset
    - python -m src.cli.manage index rebuild --no-bm25
    """
    setup_logging()
    
    from src.indexing.indexer import create_indexer
    from src.indexing.vector_store.hybrid_retriever import BM25Index
    
    typer.echo("\nğŸ”§ å‡†å¤‡é‡å»ºç´¢å¼•...\n")
    
    # åˆ›å»ºBM25ç´¢å¼•ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    bm25_index = BM25Index() if enable_bm25 else None
    
    # åˆ›å»ºç´¢å¼•å™¨
    indexer = create_indexer(bm25_index=bm25_index)
    
    # é‡å»ºç´¢å¼•
    try:
        stats = indexer.rebuild_index(reset=reset, update_bm25=enable_bm25)
        
        # æ˜¾ç¤ºç»“æœ
        typer.echo(f"\n{'='*60}")
        typer.echo(f"âœ… ç´¢å¼•é‡å»ºå®Œæˆï¼")
        typer.echo(f"{'='*60}")
        typer.echo(f"æ–‡æ¡£æ€»æ•°: {stats['total_documents']}")
        typer.echo(f"æˆåŠŸç´¢å¼•: {stats['success']}")
        typer.echo(f"å¤±è´¥: {stats['failed']}")
        typer.echo(f"æ€»Chunks: {stats['total_chunks']}")
        
        if stats['errors']:
            typer.echo(f"\né”™è¯¯è¯¦æƒ…:")
            for error in stats['errors']:
                typer.echo(f"  - {error}")
        
        # æ˜¾ç¤ºå­˜å‚¨ç»Ÿè®¡
        chroma_stats = indexer.chroma_store.get_stats()
        typer.echo(f"\nğŸ“Š å­˜å‚¨ç»Ÿè®¡:")
        typer.echo(f"  - ChromaDBæ€»Chunks: {chroma_stats['total_chunks']}")
        typer.echo(f"  - å‘é‡ç»´åº¦: {chroma_stats['vector_dimension']}")
        
        # æ˜¾ç¤ºEmbeddingç»Ÿè®¡
        embed_stats = indexer.embedder.get_stats()
        typer.echo(f"\nğŸ’° Embeddingæˆæœ¬:")
        typer.echo(f"  - æ€»Tokens: {embed_stats['total_tokens']}")
        typer.echo(f"  - ä¼°ç®—æˆæœ¬: ${embed_stats['estimated_cost_usd']:.6f}")
        
        typer.echo(f"{'='*60}\n")
        
    except Exception as e:
        typer.echo(f"\nâŒ ç´¢å¼•é‡å»ºå¤±è´¥: {e}\n")
        raise typer.Exit(code=1)


@index_app.command("test-search")
def index_test_search(
    query: str = typer.Argument(..., help="æŸ¥è¯¢å­—ç¬¦ä¸²"),
    n_results: int = typer.Option(5, "--top-k", help="è¿”å›ç»“æœæ•°é‡"),
    company: Optional[str] = typer.Option(None, "--company", help="æŒ‰å…¬å¸è¿‡æ»¤"),
    category: Optional[str] = typer.Option(None, "--category", help="æŒ‰ç±»åˆ«è¿‡æ»¤ï¼ˆLiability/Exclusion/Process/Definitionï¼‰"),
    use_hybrid: bool = typer.Option(False, "--hybrid", help="ä½¿ç”¨æ··åˆæ£€ç´¢ï¼ˆDense + BM25ï¼‰"),
):
    """
    æµ‹è¯•å‘é‡æ£€ç´¢
    
    å¯¹ç´¢å¼•æ‰§è¡Œæµ‹è¯•æ£€ç´¢ï¼ŒæŸ¥çœ‹è¿”å›ç»“æœã€‚
    
    ç¤ºä¾‹:
    - python -m src.cli.manage index test-search "ä¿é™©æœŸé—´å¤šä¹…"
    - python -m src.cli.manage index test-search "é…’é©¾èµ”å—" --category Exclusion
    - python -m src.cli.manage index test-search "ä¿é™©æœŸé—´90å¤©" --hybrid
    """
    setup_logging()
    
    from src.indexing.embedding.bge import get_embedder
    from src.indexing.vector_store.chroma import get_chroma_store
    from rich.console import Console
    from rich.table import Table
    from rich.panel import Panel
    from rich.syntax import Syntax
    
    console = Console()
    
    console.print(f"\nğŸ” æœç´¢æŸ¥è¯¢: [bold cyan]{query}[/bold cyan]\n")
    
    try:
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        embedder = get_embedder()
        query_embedding = embedder.embed_single(query)
        
        # æ„å»ºè¿‡æ»¤æ¡ä»¶
        where = {}
        if company:
            where['company'] = company
        if category:
            where['category'] = category
        
        # æ‰§è¡Œæ£€ç´¢
        if use_hybrid:
            # æ··åˆæ£€ç´¢
            console.print("ğŸ“Š ä½¿ç”¨æ··åˆæ£€ç´¢ï¼ˆDense Vector + BM25ï¼‰\n")
            
            from src.indexing.vector_store.hybrid_retriever import BM25Index, create_hybrid_retriever
            from src.common.models import PolicyChunk
            
            chroma_store = get_chroma_store()
            
            # åŠ è½½BM25ç´¢å¼•ï¼ˆéœ€è¦å…ˆæ„å»ºï¼‰
            # ç®€åŒ–ç‰ˆï¼šç›´æ¥ä½¿ç”¨Denseæ£€ç´¢
            console.print("[yellow]æ³¨æ„: æ··åˆæ£€ç´¢éœ€è¦å…ˆè¿è¡Œ `index rebuild --enable-bm25`[/yellow]\n")
            results = chroma_store.search(query_embedding, n_results=n_results, where=where if where else None)
        else:
            # çº¯å‘é‡æ£€ç´¢
            console.print("ğŸ¯ ä½¿ç”¨Dense Vectoræ£€ç´¢\n")
            chroma_store = get_chroma_store()
            results = chroma_store.search(query_embedding, n_results=n_results, where=where if where else None)
        
        # æ˜¾ç¤ºç»“æœ
        if not results:
            console.print("[yellow]æœªæ‰¾åˆ°åŒ¹é…ç»“æœ[/yellow]\n")
            return
        
        console.print(f"æ‰¾åˆ° [bold green]{len(results)}[/bold green] ä¸ªç»“æœ:\n")
        
        for i, result in enumerate(results, 1):
            metadata = result['metadata']
            distance = result.get('distance', 0)
            similarity = 1 - distance  # ä½™å¼¦ç›¸ä¼¼åº¦
            
            # åˆ›å»ºç»“æœé¢æ¿
            panel_content = f"""
[bold]ç›¸ä¼¼åº¦:[/bold] {similarity:.4f}
[bold]ç±»åˆ«:[/bold] {metadata.get('category', 'N/A')}
[bold]ç« èŠ‚:[/bold] {metadata.get('section_title', 'N/A')}
[bold]ç¼–å·:[/bold] {metadata.get('section_id', 'N/A')}

[bold]å†…å®¹:[/bold]
{result['document'][:300]}...
"""
            
            console.print(Panel(
                panel_content,
                title=f"ç»“æœ #{i}",
                border_style="green" if i == 1 else "blue"
            ))
            console.print()
        
    except Exception as e:
        console.print(f"\n[red]âŒ æ£€ç´¢å¤±è´¥: {e}[/red]\n")
        raise typer.Exit(code=1)


@index_app.command("stats")
def index_stats():
    """
    æ˜¾ç¤ºç´¢å¼•ç»Ÿè®¡ä¿¡æ¯
    
    ç¤ºä¾‹:
    - python -m src.cli.manage index stats
    """
    setup_logging()
    
    from src.indexing.vector_store.chroma import get_chroma_store
    from rich.console import Console
    from rich.table import Table
    
    console = Console()
    
    console.print("\nğŸ“Š ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯\n")
    
    try:
        chroma_store = get_chroma_store()
        stats = chroma_store.get_stats()
        
        # åˆ›å»ºè¡¨æ ¼
        table = Table(title="ChromaDBç»Ÿè®¡")
        table.add_column("æŒ‡æ ‡", style="cyan")
        table.add_column("å€¼", style="green")
        
        table.add_row("Collectionåç§°", stats['collection_name'])
        table.add_row("æ€»Chunksæ•°", str(stats['total_chunks']))
        table.add_row("å‘é‡ç»´åº¦", str(stats['vector_dimension']))
        table.add_row("è·ç¦»åº¦é‡", stats['distance_metric'])
        table.add_row("æŒä¹…åŒ–ç›®å½•", stats['persist_directory'])
        
        console.print(table)
        console.print()
        
    except Exception as e:
        console.print(f"\n[red]âŒ è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}[/red]\n")
        raise typer.Exit(code=1)


if __name__ == "__main__":
    app()
