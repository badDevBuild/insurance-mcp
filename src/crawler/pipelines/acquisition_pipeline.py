"""
å®Œæ•´çš„æ•°æ®é‡‡é›†ç®¡é“
åŒ…æ‹¬ï¼šäº§å“å‘ç° -> PDFä¸‹è½½ -> å…ƒæ•°æ®ä¿å­˜åˆ°æ•°æ®åº“
"""
import asyncio
import hashlib
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime

from src.crawler.discovery.pingan_life_spider import PingAnLifeSpider
from src.crawler.acquisition.downloader import PDFDownloader
from src.common.models import Product, PolicyDocument, VerificationStatus
from src.common.repository import SQLiteRepository
from src.common.logging import logger
from src.common.config import config


class AcquisitionPipeline:
    """
    å®Œæ•´çš„æ•°æ®é‡‡é›†ç®¡é“
    
    æµç¨‹:
    1. çˆ¬å–äº§å“åˆ—è¡¨å’ŒPDFé“¾æ¥
    2. ä¿å­˜äº§å“å…ƒæ•°æ®åˆ°æ•°æ®åº“
    3. ä¸‹è½½PDFæ–‡ä»¶åˆ°æœ¬åœ°
    4. è®¡ç®—æ–‡ä»¶å“ˆå¸Œå¹¶æ›´æ–°æ•°æ®åº“
    5. å»é‡å’Œé”™è¯¯å¤„ç†
    """
    
    def __init__(self, company: str = "å¹³å®‰äººå¯¿"):
        self.company = company
        self.spider = PingAnLifeSpider(headless=True)
        self.downloader = PDFDownloader(max_retries=3, initial_delay=1.0)
        self.repo = SQLiteRepository()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "products_discovered": 0,
            "products_new": 0,
            "products_existing": 0,
            "pdfs_total": 0,
            "pdfs_downloaded": 0,
            "pdfs_skipped": 0,
            "pdfs_failed": 0,
        }
    
    async def run(self, limit: int = 100, fetch_details: bool = True) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´çš„é‡‡é›†æµç¨‹
        
        Args:
            limit: æœ€å¤§çˆ¬å–äº§å“æ•°é‡
            fetch_details: æ˜¯å¦è·å–PDFé“¾æ¥
            
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        logger.info(f"ğŸš€ å¼€å§‹é‡‡é›† {self.company} çš„äº§å“æ•°æ®...")
        logger.info(f"é…ç½®: limit={limit}, fetch_details={fetch_details}")
        
        # æ­¥éª¤1: çˆ¬å–äº§å“åˆ—è¡¨
        logger.info("=" * 80)
        logger.info("æ­¥éª¤ 1/3: çˆ¬å–äº§å“åˆ—è¡¨")
        logger.info("=" * 80)
        products_data = await self.spider.discover_products(
            limit=limit,
            fetch_details=fetch_details
        )
        self.stats["products_discovered"] = len(products_data)
        logger.info(f"âœ“ å‘ç° {len(products_data)} ä¸ªäº§å“")
        
        if not products_data:
            logger.warning("æœªå‘ç°ä»»ä½•äº§å“ï¼Œæµç¨‹ç»ˆæ­¢")
            return self.stats
        
        # æ­¥éª¤2: ä¿å­˜äº§å“å…ƒæ•°æ®å¹¶ä¸‹è½½PDF
        logger.info("=" * 80)
        logger.info("æ­¥éª¤ 2/3: ä¿å­˜äº§å“å…ƒæ•°æ®")
        logger.info("=" * 80)
        
        for idx, product_data in enumerate(products_data, 1):
            logger.info(f"\n[{idx}/{len(products_data)}] å¤„ç†äº§å“: {product_data['name']} ({product_data['product_code']})")
            
            try:
                # 2.1 ä¿å­˜æˆ–æ›´æ–°äº§å“ä¿¡æ¯
                product = await self._save_product(product_data)
                
                # 2.2 ä¸‹è½½PDFæ–‡ä»¶
                if fetch_details and product_data.get('pdf_links'):
                    await self._download_pdfs(product, product_data['pdf_links'])
                
            except Exception as e:
                logger.error(f"âŒ å¤„ç†äº§å“å¤±è´¥: {product_data['name']}: {e}")
                import traceback
                logger.debug(traceback.format_exc())
                continue
        
        # æ­¥éª¤3: è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        logger.info("=" * 80)
        logger.info("æ­¥éª¤ 3/3: é‡‡é›†å®Œæˆ")
        logger.info("=" * 80)
        self._print_stats()
        
        return self.stats
    
    async def _save_product(self, product_data: Dict[str, Any]) -> Product:
        """
        ä¿å­˜æˆ–æ›´æ–°äº§å“ä¿¡æ¯
        
        Args:
            product_data: çˆ¬å–åˆ°çš„äº§å“æ•°æ®
            
        Returns:
            Productå®ä¾‹
        """
        product_code = product_data['product_code']
        
        # æ£€æŸ¥äº§å“æ˜¯å¦å·²å­˜åœ¨
        existing_product = self.repo.get_product_by_code(product_code, self.company)
        
        if existing_product:
            logger.info(f"  âœ“ äº§å“å·²å­˜åœ¨: {existing_product.id}")
            self.stats["products_existing"] += 1
            return existing_product
        
        # åˆ›å»ºæ–°äº§å“
        product = Product(
            product_code=product_code,
            name=product_data['name'],
            company=self.company,
            publish_time=product_data.get('publish_time'),
            category=None  # å¯ä»¥ä»äº§å“åç§°æ¨æ–­
        )
        
        self.repo.add_product(product)
        logger.info(f"  âœ“ æ–°äº§å“å·²ä¿å­˜: {product.id}")
        self.stats["products_new"] += 1
        
        return product
    
    async def _download_pdfs(self, product: Product, pdf_links: Dict[str, str]):
        """
        ä¸‹è½½äº§å“çš„æ‰€æœ‰PDFæ–‡ä»¶
        
        Args:
            product: Productå®ä¾‹
            pdf_links: PDFé“¾æ¥å­—å…¸ {æ–‡æ¡£ç±»å‹: URL}
        """
        logger.info(f"  ğŸ“„ ä¸‹è½½PDFæ–‡ä»¶: {len(pdf_links)} ä¸ªæ–‡æ¡£")
        
        for doc_type, url in pdf_links.items():
            # Filter for supported document types only
            # This ensures we only save documents that match our DocumentType enum
            if doc_type not in ["äº§å“æ¡æ¬¾", "äº§å“è¯´æ˜ä¹¦", "äº§å“è´¹ç‡è¡¨"]:
                logger.debug(f"    âŠ™ è·³è¿‡ä¸æ”¯æŒçš„æ–‡æ¡£ç±»å‹: {doc_type}")
                continue
                
            self.stats["pdfs_total"] += 1
            
            try:
                # æ£€æŸ¥æ˜¯å¦å·²ä¸‹è½½è¿‡ï¼ˆæ ¹æ®URLå»é‡ï¼‰
                if self._is_document_exists(product.id, doc_type, url):
                    logger.info(f"    âŠ™ è·³è¿‡ [{doc_type}]: å·²å­˜åœ¨")
                    self.stats["pdfs_skipped"] += 1
                    continue
                
                # æ„å»ºä¿å­˜è·¯å¾„
                save_path = self._get_save_path(product, doc_type, url)
                
                # ä¸‹è½½PDF
                logger.info(f"    â†“ ä¸‹è½½ [{doc_type}]...")
                success = await self.downloader.download(url, save_path)
                
                if success:
                    # è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
                    file_hash = self._calculate_file_hash(save_path)
                    file_size = save_path.stat().st_size
                    
                    # ä¿å­˜æ–‡æ¡£è®°å½•
                    doc = PolicyDocument(
                        product_id=product.id,
                        doc_type=doc_type,
                        filename=save_path.name,
                        local_path=str(save_path),
                        url=url,
                        file_hash=file_hash,
                        file_size=file_size,
                        downloaded_at=datetime.now(),
                        verification_status=VerificationStatus.PENDING,
                        pdf_links=pdf_links  # ä¿å­˜æ‰€æœ‰PDFé“¾æ¥ä»¥å®ç°å¯è¿½æº¯æ€§
                    )
                    
                    self.repo.add_document(doc)
                    logger.info(f"    âœ“ å·²ä¿å­˜ [{doc_type}]: {file_size / 1024:.1f} KB")
                    self.stats["pdfs_downloaded"] += 1
                else:
                    logger.warning(f"    âœ— ä¸‹è½½å¤±è´¥ [{doc_type}]")
                    self.stats["pdfs_failed"] += 1
                    
            except Exception as e:
                logger.error(f"    âœ— å¤„ç†æ–‡æ¡£å¤±è´¥ [{doc_type}]: {e}")
                self.stats["pdfs_failed"] += 1
    
    def _is_document_exists(self, product_id: str, doc_type: str, url: str) -> bool:
        """
        æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å·²å­˜åœ¨
        
        Args:
            product_id: äº§å“ID
            doc_type: æ–‡æ¡£ç±»å‹
            url: æ–‡æ¡£URL
            
        Returns:
            æ˜¯å¦å·²å­˜åœ¨
        """
        # é€šè¿‡product_id + doc_type + urlç»„åˆåˆ¤æ–­
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…å¯ä»¥æŸ¥è¯¢æ•°æ®åº“
        try:
            with self.repo.get_db_connection() as conn:
                cursor = conn.cursor()
                result = cursor.execute(
                    "SELECT id FROM policy_documents WHERE product_id = ? AND doc_type = ? AND url = ?",
                    (product_id, doc_type, url)
                ).fetchone()
                return result is not None
        except:
            return False
    
    def _get_save_path(self, product: Product, doc_type: str, url: str) -> Path:
        """
        ç”ŸæˆPDFä¿å­˜è·¯å¾„
        
        è·¯å¾„ç»“æ„: data/raw/{company}/{product_code}/{doc_type}.pdf
        
        Args:
            product: Productå®ä¾‹
            doc_type: æ–‡æ¡£ç±»å‹
            url: æ–‡æ¡£URL
            
        Returns:
            ä¿å­˜è·¯å¾„
        """
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        company_dir = config.RAW_DATA_DIR / self._sanitize_filename(self.company)
        product_dir = company_dir / self._sanitize_filename(product.product_code)
        product_dir.mkdir(parents=True, exist_ok=True)
        
        # æ–‡ä»¶åï¼šæ–‡æ¡£ç±»å‹.pdf
        filename = f"{self._sanitize_filename(doc_type)}.pdf"
        
        return product_dir / filename
    
    def _sanitize_filename(self, name: str) -> str:
        """
        æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤éæ³•å­—ç¬¦
        
        Args:
            name: åŸå§‹åç§°
            
        Returns:
            æ¸…ç†åçš„åç§°
        """
        # æ›¿æ¢éæ³•å­—ç¬¦
        illegal_chars = ['/', '\\', ':', '*', '?', '"', '<', '>', '|']
        for char in illegal_chars:
            name = name.replace(char, '_')
        return name.strip()
    
    def _calculate_file_hash(self, file_path: Path) -> str:
        """
        è®¡ç®—æ–‡ä»¶çš„SHA-256å“ˆå¸Œå€¼
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            å“ˆå¸Œå€¼ï¼ˆåå…­è¿›åˆ¶å­—ç¬¦ä¸²ï¼‰
        """
        sha256 = hashlib.sha256()
        
        with open(file_path, 'rb') as f:
            while chunk := f.read(8192):
                sha256.update(chunk)
        
        return sha256.hexdigest()
    
    def _print_stats(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ“Š é‡‡é›†ç»Ÿè®¡")
        logger.info("=" * 80)
        logger.info(f"äº§å“:")
        logger.info(f"  - å‘ç°: {self.stats['products_discovered']} ä¸ª")
        logger.info(f"  - æ–°å¢: {self.stats['products_new']} ä¸ª")
        logger.info(f"  - å·²å­˜åœ¨: {self.stats['products_existing']} ä¸ª")
        logger.info(f"\nPDFæ–‡æ¡£:")
        logger.info(f"  - æ€»è®¡: {self.stats['pdfs_total']} ä¸ª")
        logger.info(f"  - å·²ä¸‹è½½: {self.stats['pdfs_downloaded']} ä¸ª")
        logger.info(f"  - å·²è·³è¿‡: {self.stats['pdfs_skipped']} ä¸ª")
        logger.info(f"  - å¤±è´¥: {self.stats['pdfs_failed']} ä¸ª")
        logger.info("=" * 80)
        
        if self.stats['pdfs_failed'] > 0:
            logger.warning(f"âš ï¸  æœ‰ {self.stats['pdfs_failed']} ä¸ªPDFä¸‹è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")


# ä¾¿æ·å‡½æ•°
async def run_acquisition(company: str = "å¹³å®‰äººå¯¿", limit: int = 100) -> Dict[str, Any]:
    """
    è¿è¡Œæ•°æ®é‡‡é›†æµç¨‹ï¼ˆä¾¿æ·å‡½æ•°ï¼‰
    
    Args:
        company: ä¿é™©å…¬å¸åç§°
        limit: æœ€å¤§çˆ¬å–äº§å“æ•°é‡
        
    Returns:
        ç»Ÿè®¡ä¿¡æ¯å­—å…¸
    """
    pipeline = AcquisitionPipeline(company=company)
    stats = await pipeline.run(limit=limit, fetch_details=True)
    return stats


if __name__ == "__main__":
    # æµ‹è¯•è¿è¡Œ
    async def test():
        # åˆå§‹åŒ–æ•°æ®åº“
        from src.common.db import init_db
        init_db()
        
        # è¿è¡Œé‡‡é›†ï¼ˆåªé‡‡é›†5ä¸ªäº§å“ç”¨äºæµ‹è¯•ï¼‰
        stats = await run_acquisition(company="å¹³å®‰äººå¯¿", limit=5)
        
        print("\nâœ… æµ‹è¯•å®Œæˆï¼")
        print(f"è¯·æ£€æŸ¥: data/raw/å¹³å®‰äººå¯¿/ ç›®å½•ä¸‹çš„PDFæ–‡ä»¶")
    
    asyncio.run(test())

