# 功能规格书：保险 MCP 核心平台 (Insurance MCP Core Platform)

**功能分支**: `001-insurance-mcp-core`
**创建日期**: 2025-11-20
**最后更新**: 2025-11-21
**状态**: 草稿 (Draft)
**输入来源**: 用户描述："构建一个高准确度、可信赖的保险领域垂直信息 MCP 服务器..."
**最新变更**: 2025-11-21 - 调整爬虫策略为按保险公司维度采集，第一期聚焦平安人寿

## 澄清 (Clarifications)

### 会话 2025-11-20
- Q: 选择哪种向量数据库方案？ → A: **ChromaDB (Local)** - 轻量级、开源、支持进程内运行或 Docker，适合第一阶段开发，零成本且易于本地调试。
- Q: Embedding 模型选择方案？ → A: **OpenAI API (text-embedding-3-small)** - 行业标准，低成本（$0.02/1M tokens），即用即付，适合 MVP 快速验证准确性。

## 用户场景与测试 (User Scenarios & Testing)

### 用户故事 1 - AI 客户端获取精准信息 (AI Client Accurate Information Retrieval) (优先级: P1)

作为 AI 客户端（通过 LLM），我希望能够检索到基于语义搜索的准确保险条款，从而能够在不产生幻觉的情况下回答用户问题。

**优先级理由**: 这是核心价值主张——消除保险咨询中的幻觉问题。

**独立测试**: 通过发送针对特定保险责任细节的 MCP 请求，验证返回内容是否包含条款原文的精确文本。

**验收场景 (Acceptance Scenarios)**:

1. **Given (已知)** 数据库中存在已核验的保险条款，**When (当)** AI 客户端调用 `search_products` 进行模糊查询时（例如："产品 X 承保攀岩吗？"），**Then (那么)** 系统返回具有高语义相似度的相关条款。
2. **Given (已知)** 查询的产品存在，但没有与查询相关的条款，**When (当)** 进行搜索时，**Then (那么)** 系统返回空结果或"未找到相关条款"，而不是编造答案。
3. **Given (已知)** 发起查询，**When (当)** 返回结果时，**Then (那么)** 每个结果都包含指向具体文件和章节的 `source_reference`（来源引用）。

---

### 用户故事 2 - 数据审核员核验 (Data Auditor Verification) (优先级: P1)

作为数据审核员，我希望能够将转换后的 Markdown 文本与原始 PDF 进行对比审核，从而确保上线数据的 100% 准确性。

**优先级理由**: 为了满足"准确性高于一切"的原则，必须有"人机协同 (Human-in-the-Loop)"环节。

**独立测试**: 运行审核工具/脚本，查看差异对比界面，并批准一个文档。

**验收场景 (Acceptance Scenarios)**:

1. **Given (已知)** 一个新爬取并转换完成的保单文档，**When (当)** 审核员打开审核界面 (CLI/Web) 时，**Then (那么)** 看到状态为"未核验 (Unverified)"。
2. **Given (已知)** 一个未核验的文档，**When (当)** 审核员将其标记为"已核验 (Verified)"，**Then (那么)** 该文档即可进入向量索引流程。
3. **Given (已知)** 一个存在转换错误的文档，**When (当)** 审核员标记报错，**Then (那么)** 该文档在修复前会被排除在索引之外。

---

### 用户故事 3 - 自动化条款采集 (Automated Policy Acquisition) (优先级: P2)

作为系统运维人员，我希望系统能按保险公司维度自动发现并获取该公司的最新保险条款，从而建立精准的公司产品数据库。

**优先级理由**: 确保"单一事实来源"的覆盖率和时效性。第一期聚焦平安人寿，从该公司官网公开信息披露栏目直接采集，验证端到端流程可行性。

**独立测试**: 针对平安人寿官网 (https://life.pingan.com/gongkaixinxipilu/baoxianchanpinmulujitiaokuan.jsp)，验证爬虫能正确解析产品目录并下载对应的 PDF 文件。

**验收场景 (Acceptance Scenarios)**:

1. **Given (已知)** 平安人寿官网更新了产品目录，**When (当)** "发现层"任务运行时，**Then (那么)** 系统能提取产品元数据（产品代码、产品名称、报备材料链接、发布时间）并建立索引条目。
2. **Given (已知)** 建立了新的索引条目，**When (当)** "采集层"任务运行时，**Then (那么)** 系统从平安人寿官网下载 PDF 文件，按产品代码去重存储。
3. **Given (已知)** 目标网站有反爬机制（如 WAF），**When (当)** 爬虫遇到 403/429 错误时，**Then (那么)** 系统自动执行指数退避 (exponential backoff) 并记录日志，而不是持续请求导致 IP 被封。

### 边界情况 (Edge Cases)

- **EC-001 - 加密/受保护的 PDF**: 如果下载的 PDF 有密码保护，系统尝试使用通用策略（如空密码）解密；若失败，标记为"处理失败"并告警。
- **EC-002 - 低质量/双栏扫描件**: 对于 OCR 置信度低或双栏排版的扫描件，系统必须使用支持版面分析 (Layout Analysis) 的解析引擎，防止文字跨栏乱序拼接；若无法处理，标记为需要人工审核。
- **EC-003 - 爬虫被封锁**: 严格遵守 QPS 限制。如果目标网站拦截，系统执行熔断机制，暂停该域名的采集任务 5 分钟以上。
- **EC-004 - 隐私数据避让**: 爬虫必须配置 URL 过滤器，**严禁**访问包含 `policy` (保单查询)、`user` (用户信息) 等路径的接口，防止触碰合规红线。

---

## 需求 (Requirements)

### 功能性需求 (Functional Requirements)

- **FR-001**: 系统必须提供 `search_products(query)` MCP 工具，支持自然语言查询，并可按保险公司或产品名称过滤。
- **FR-002**: 系统必须提供 `get_product_terms(product_id)` MCP 工具，用于返回特定条款的完整结构化文本。
- **FR-003**: 系统必须实现**分层爬虫架构**：
    - **发现层 (Discovery)**: 按保险公司维度，从目标保险公司官网"公开信息披露"栏目定期抓取产品元数据（产品代码、产品名称、报备材料链接）。第一期针对平安人寿官网 (https://life.pingan.com/gongkaixinxipilu/baoxianchanpinmulujitiaokuan.jsp)。
    - **采集层 (Acquisition)**: 基于元数据，从保险公司官网下载 PDF 实体文件。
- **FR-004**: 系统必须实现高保真 **PDF 转 Markdown** 转换管道：
    - 支持**版面分析 (Layout Analysis)**，正确还原双栏排版文档的阅读顺序。
    - 具备**表格还原能力**，能将简单的费率表、利益演示表转换为 Markdown 表格，保持行列结构不崩坏。
- **FR-005**: 系统必须提供机制（CLI 或简易 UI），供人工审核员将文档转换质量标记为"已核验 (Verified)"或"已驳回 (Rejected)"。
- **FR-006**: 系统必须**仅**将标记为"已核验"的文档索引到向量数据库中。
- **FR-007**: 系统必须持久化存储原始 PDF 文件，并将解析后的文本块与原始文件关联，以确可追溯性。
- **FR-008**: 爬虫必须内置**合规限制**：全局配置最大 QPS（每秒请求数），并强制遵守目标站点的 Robots 协议（针对非强制披露目录）。

### 关键实体 (Key Entities)

- **Product (产品)**: 代表逻辑上的保险产品（如："平安福耀年金保险"）。
  - 属性：`id`, `product_code` (产品代码，用于去重), `name`, `company`, `category`, `publish_time`, `created_at`。
  
- **PolicyDocument (条款文档)**: 代表单个 PDF 文件（如产品条款、费率表、说明书）。
  - 属性：`id`, `product_id`, `doc_type` (文档类型), `filename`, `local_path`, `url`, `file_hash`, `file_size`, `downloaded_at`, `verification_status` (PENDING, VERIFIED, REJECTED), `pdf_links` (所有PDF链接，用于来源追溯)。
  - **可追溯性**: `pdf_links` 字段以JSON格式保存产品的所有文档链接（如 `{"产品条款": "url1", "费率表": "url2"}`），确保符合Constitution 2.2原则。
  
- **PolicyChunk (条款切片)**: 用于向量索引的文本段。
  - 属性：`id`, `document_id`, `content`, `embedding_vector`, `metadata` (包含页码、章节标题等)。

## 成功标准 (Success Criteria)

### 可衡量的结果 (Measurable Outcomes)

- **SC-001**: MCP API 返回的数据 **100%** 关联至"已核验"的源文档。
- **SC-002**: 爬虫能在发布后 **24小时** 内成功识别并下载目标站点的新 PDF（取决于运行频率）。
- **SC-003**: 对于包含 50 个标准保险问题的"黄金测试集"，向量搜索能在第 1 位结果中返回正确条款的比例达到 **90%**。
- **SC-004**: 系统能将已核验文档中的标准保单表格（如利益演示表）解析为可读 Markdown 表格，行列完整性达到 **100%**。

## 假设与依赖 (Assumptions & Dependencies)

- **假设**: 平安人寿官网"公开信息披露"栏目允许爬取，通过标准 HTTP 请求可访问公开文档库（无阻挡基本访问的复杂验证码）。
- **假设**: 保险条款主要是基于文本的 PDF，而非扫描图片（虽然提到了 OCR，但为了第一阶段效率，优先处理原生数字 PDF）。
- **假设**: 第一期仅实现平安人寿公司的爬虫，架构设计需考虑未来扩展到其他保险公司的可扩展性。
- **依赖**: 访问 **OpenAI API** (用于 `text-embedding-3-small` 模型) 进行向量化。
- **依赖**: **ChromaDB (Local)** 作为向量数据库，运行于本地环境或 Docker 中。
