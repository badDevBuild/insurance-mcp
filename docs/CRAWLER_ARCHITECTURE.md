# çˆ¬è™«æ¶æ„è¯´æ˜

## ğŸ“š å½“å‰ä»£ç é€»è¾‘è¯¦è§£

### 1. **PingAnLifeSpider ç±»ç»“æ„**

```python
class PingAnLifeSpider:
    BASE_URL = "https://life.pingan.com/gongkaixinxipilu/..."
    
    def __init__(self, headless=True):
        # åˆå§‹åŒ–ï¼šè®¾ç½®æµè§ˆå™¨æ¨¡å¼
    
    async def discover_products(self, limit, fetch_details):
        # ä¸»æµç¨‹ï¼šçˆ¬å–äº§å“åˆ—è¡¨
```

### 2. **æ‰§è¡Œæµç¨‹å›¾**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. å¯åŠ¨æµè§ˆå™¨ (Chromium)                    â”‚
â”‚    - è®¾ç½®User-Agent                         â”‚
â”‚    - è®¾ç½®è§†å£å¤§å°                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. å¯¼èˆªåˆ°ç›®æ ‡ç½‘é¡µ                           â”‚
â”‚    await page.goto(BASE_URL)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. ç­‰å¾…è¡¨æ ¼åŠ è½½                             â”‚
â”‚    await page.wait_for_selector("table")    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. å¾ªç¯å¤„ç†æ¯ä¸€é¡µ                           â”‚
â”‚    while len(results) < limit:              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. è·å–å½“å‰é¡µçš„æ‰€æœ‰è¡Œ                        â”‚
â”‚    rows = await page.locator("tr").all()    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. é€è¡Œæå–æ•°æ®                             â”‚
â”‚    for row in rows:                         â”‚
â”‚      - äº§å“ä»£ç  (tds[0])                    â”‚
â”‚      - äº§å“åç§° (tds[1])                    â”‚
â”‚      - ä¸‹æ‹‰èœå• (tds[2]) â† PDFé“¾æ¥          â”‚
â”‚      - å‘å¸ƒæ—¶é—´ (tds[3])                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 7. æå–PDFé“¾æ¥ (æ ¸å¿ƒé€»è¾‘)                   â”‚
â”‚    ul_element = tds[2].locator("ul")        â”‚
â”‚    links = ul_element.locator("a").all()    â”‚
â”‚                                             â”‚
â”‚    å…³é”®ï¼šä¸éœ€è¦ç‚¹å‡»å±•å¼€ä¸‹æ‹‰èœå•ï¼           â”‚
â”‚    åŸå› ï¼šæ‰€æœ‰é“¾æ¥å·²åœ¨DOMä¸­ï¼Œåªæ˜¯CSSéšè—     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 8. æ„å»ºè¿”å›æ•°æ®                             â”‚
â”‚    item = {                                 â”‚
â”‚      "product_code": "2124",                â”‚
â”‚      "name": "å¹³å®‰ç¦è€€å¹´é‡‘ä¿é™©",             â”‚
â”‚      "pdf_links": {                         â”‚
â”‚        "äº§å“æ¡æ¬¾": "https://...",           â”‚
â”‚        "å¤‡æ¡ˆäº§å“æ¸…å•è¡¨": "https://...",     â”‚
â”‚        ...                                  â”‚
â”‚      }                                      â”‚
â”‚    }                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 9. ç¿»é¡µå¤„ç†                                 â”‚
â”‚    next_btn = page.locator("a:ä¸‹ä¸€é¡µ")     â”‚
â”‚    await next_btn.click()                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 10. è¿”å›ç»“æœ                                â”‚
â”‚     return results                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3. **æ ¸å¿ƒæŠ€å·§ï¼šç›´æ¥è¯»å–éšè—çš„DOMå…ƒç´ **

```python
# âŒ é”™è¯¯æ–¹å¼ï¼šå°è¯•ç‚¹å‡»å±•å¼€ä¸‹æ‹‰èœå•
await dropdown.click()
await asyncio.sleep(1)
links = await page.locator("ul a").all()

# âœ… æ­£ç¡®æ–¹å¼ï¼šç›´æ¥è¯»å–DOMä¸­çš„é“¾æ¥
ul_element = dropdown_cell.locator("ul").first  # <ul> åœ¨DOMä¸­å­˜åœ¨
links = await ul_element.locator("a").all()     # ç›´æ¥è·å–æ‰€æœ‰ <a>

# åŸå› ï¼šHTMLç»“æ„å¦‚ä¸‹
# <td class="dropdown">
#   <a>è¯·é€‰æ‹©</a>
#   <ul style="display: none;">  â† CSSéšè—ï¼Œä½†DOMä¸­å­˜åœ¨
#     <a href="...">äº§å“æ¡æ¬¾</a>
#     <a href="...">å¤‡æ¡ˆäº§å“æ¸…å•è¡¨</a>
#   </ul>
# </td>
```

---

## ğŸ”§ å¦‚ä½•æ·»åŠ å…¶ä»–ä¿é™©å…¬å¸

### æ–¹æ¡ˆæ¦‚è§ˆ

æˆ‘å·²ç»ä¸ºæ‚¨åˆ›å»ºäº†å®Œæ•´çš„å¯æ‰©å±•æ¶æ„ï¼š

```
æ¶æ„ç»„ä»¶:
â”œâ”€â”€ base_spider.py              â† åŸºç±»ï¼ˆå®šä¹‰é€šç”¨æ¥å£ï¼‰
â”œâ”€â”€ pingan_life_spider.py       â† å¹³å®‰äººå¯¿å®ç°ï¼ˆæ‚¨å½“å‰ä½¿ç”¨çš„ï¼‰
â”œâ”€â”€ pingan_life_spider_v2.py    â† é‡æ„ç‰ˆï¼ˆç»§æ‰¿åŸºç±»ï¼‰
â”œâ”€â”€ chinalifeinsurance_spider.py â† å…¶ä»–å…¬å¸æ¨¡æ¿
â”œâ”€â”€ spider_factory.py           â† å·¥å‚ç±»ï¼ˆç»Ÿä¸€ç®¡ç†ï¼‰
â””â”€â”€ [æ–°å…¬å¸]_spider.py          â† æ‚¨è¦æ·»åŠ çš„æ–°çˆ¬è™«
```

### æ·»åŠ æ–°å…¬å¸çš„5æ­¥æµç¨‹

#### **æ­¥éª¤1: åˆ›å»ºçˆ¬è™«æ–‡ä»¶**

```bash
cd src/crawler/discovery/
cp chinalifeinsurance_spider.py taikang_life_spider.py  # å¤åˆ¶æ¨¡æ¿
```

#### **æ­¥éª¤2: ä¿®æ”¹ç±»å®šä¹‰**

```python
class TaikangLifeSpider(BaseInsuranceSpider):
    """æ³°åº·äººå¯¿çˆ¬è™«"""
    
    # 1ï¸âƒ£ è®¾ç½®ç›®æ ‡ç½‘ç«™URL
    BASE_URL = "https://www.taikanglife.com/products/list"
    
    # 2ï¸âƒ£ è®¾ç½®å…¬å¸åç§°
    COMPANY_NAME = "æ³°åº·äººå¯¿"
```

#### **æ­¥éª¤3: åˆ†æç›®æ ‡ç½‘ç«™**

ä½¿ç”¨æµè§ˆå™¨å¼€å‘è€…å·¥å…· (F12) åˆ†æé¡µé¢ç»“æ„ï¼š

```python
# ç¤ºä¾‹ï¼šå‡è®¾æ³°åº·äººå¯¿çš„HTMLç»“æ„æ˜¯
# <div class="product-list">
#   <div class="product-item">
#     <span class="code">äº§å“ä»£ç </span>
#     <h3 class="name">äº§å“åç§°</h3>
#     <span class="date">å‘å¸ƒæ—¶é—´</span>
#     <div class="files">
#       <a href="...">æ¡æ¬¾</a>
#       <a href="...">è´¹ç‡è¡¨</a>
#     </div>
#   </div>
# </div>
```

#### **æ­¥éª¤4: å®ç°4ä¸ªæ ¸å¿ƒæ–¹æ³•**

```python
class TaikangLifeSpider(BaseInsuranceSpider):
    
    # æ–¹æ³•1: ç­‰å¾…é¡µé¢åŠ è½½
    async def wait_for_page_load(self, page: Page):
        await page.wait_for_selector(".product-list", timeout=10000)
    
    # æ–¹æ³•2: è§£æäº§å“åˆ—è¡¨ â­ æ ¸å¿ƒæ–¹æ³•
    async def parse_product_list(self, page, limit, fetch_details):
        results = []
        
        # è·å–æ‰€æœ‰äº§å“é¡¹
        items = await page.locator(".product-item").all()
        
        for item in items:
            # æå–å­—æ®µï¼ˆæ ¹æ®å®é™…HTMLè°ƒæ•´é€‰æ‹©å™¨ï¼‰
            code = await item.locator(".code").text_content()
            name = await item.locator(".name").text_content()
            date = await item.locator(".date").text_content()
            
            # æå–PDFé“¾æ¥
            files_div = item.locator(".files")
            pdf_links = await self.extract_pdf_links(files_div)
            
            # æ„å»ºæ•°æ®
            results.append(self.normalize_product_data(
                product_code=code,
                name=name,
                publish_time=date,
                pdf_links=pdf_links,
                source_url=pdf_links.get("æ¡æ¬¾", "")
            ))
        
        return results
    
    # æ–¹æ³•3: æå–PDFé“¾æ¥
    async def extract_pdf_links(self, element):
        pdf_links = {}
        links = await element.locator("a").all()
        
        for link in links:
            text = await link.text_content()
            url = await link.get_attribute("href")
            if text and url:
                pdf_links[text.strip()] = url
        
        return pdf_links
    
    # æ–¹æ³•4: ç¿»é¡µ (å¦‚æœéœ€è¦)
    async def go_to_next_page(self, page: Page) -> bool:
        next_btn = page.locator(".pagination .next")
        if await next_btn.count() > 0:
            await next_btn.click()
            return True
        return False
```

#### **æ­¥éª¤5: æ³¨å†Œåˆ°å·¥å‚**

åœ¨ `spider_factory.py` ä¸­æ³¨å†Œï¼š

```python
from src.crawler.discovery.taikang_life_spider import TaikangLifeSpider

# åœ¨æ–‡ä»¶æœ«å°¾æ·»åŠ 
SpiderFactory.register("taikang-life", TaikangLifeSpider)
```

### ä½¿ç”¨æ–°çˆ¬è™«

```python
from src.crawler.discovery.spider_factory import SpiderFactory

# æ–¹å¼1: é€šè¿‡å·¥å‚åˆ›å»º
spider = SpiderFactory.create("taikang-life", headless=True)
products = await spider.discover_products(limit=50)

# æ–¹å¼2: ç›´æ¥å®ä¾‹åŒ–
from src.crawler.discovery.taikang_life_spider import TaikangLifeSpider
spider = TaikangLifeSpider(headless=True)
products = await spider.discover_products(limit=50)
```

---

## ğŸ“Š å¯¹æ¯”ï¼šå½“å‰å®ç° vs é‡æ„ç‰ˆ

| ç‰¹æ€§ | å½“å‰ pingan_life_spider.py | é‡æ„ç‰ˆ (åŸºäºBaseSpider) |
|------|---------------------------|------------------------|
| **ä»£ç å¤ç”¨** | âŒ æ¯ä¸ªå…¬å¸ç‹¬ç«‹å®ç° | âœ… ç»§æ‰¿åŸºç±»ï¼Œå¤ç”¨é€šç”¨é€»è¾‘ |
| **å¯ç»´æŠ¤æ€§** | âš ï¸  ä¿®æ”¹éœ€åŒæ­¥å¤šå¤„ | âœ… ä¿®æ”¹åŸºç±»å³å¯å½±å“æ‰€æœ‰å­ç±» |
| **ç»Ÿä¸€ç®¡ç†** | âŒ éœ€è¦æ‰‹åŠ¨å¯¼å…¥å„ä¸ªçˆ¬è™« | âœ… å·¥å‚ç±»ç»Ÿä¸€ç®¡ç† |
| **æ‰©å±•æ€§** | âš ï¸  æ·»åŠ æ–°å…¬å¸éœ€è¦å¤§é‡ä»£ç  | âœ… åªéœ€å®ç°4ä¸ªæ ¸å¿ƒæ–¹æ³• |
| **æµ‹è¯•è¦†ç›–** | âš ï¸  æ¯ä¸ªçˆ¬è™«å•ç‹¬æµ‹è¯• | âœ… åŸºç±»æµ‹è¯• + å­ç±»ç‰¹å®šæµ‹è¯• |

### å»ºè®®ï¼šé€æ­¥è¿ç§»

```python
# å½“å‰ä½¿ç”¨ï¼š
from src.crawler.discovery.pingan_life_spider import PingAnLifeSpider

# æœªæ¥è¿ç§»åˆ°ï¼š
from src.crawler.discovery.spider_factory import SpiderFactory
spider = SpiderFactory.create("pingan-life")
```

---

## ğŸ¯ å¿«é€Ÿå‚è€ƒ

### å¸¸ç”¨é€‰æ‹©å™¨

| å…ƒç´ ç±»å‹ | Playwrighté€‰æ‹©å™¨ç¤ºä¾‹ |
|---------|---------------------|
| è¡¨æ ¼è¡Œ | `table tbody tr` |
| è¡¨æ ¼å•å…ƒæ ¼ | `td:nth-child(1)` |
| ç±»å | `.product-item` |
| ID | `#product-list` |
| æ–‡æœ¬å†…å®¹ | `a:has-text('ä¸‹ä¸€é¡µ')` |
| å±æ€§ | `a[href$='.pdf']` |
| åä»£å…ƒç´  | `.dropdown ul a` |

### å¸¸ç”¨æ–¹æ³•

```python
# è·å–å…ƒç´ 
element = await page.locator("selector").first
elements = await page.locator("selector").all()

# æå–æ•°æ®
text = await element.text_content()      # æ–‡æœ¬å†…å®¹
html = await element.inner_html()        # HTMLå†…å®¹
attr = await element.get_attribute("href") # å±æ€§å€¼

# äº¤äº’
await element.click()                    # ç‚¹å‡»
await element.fill("text")               # è¾“å…¥æ–‡æœ¬
await element.check()                    # å‹¾é€‰å¤é€‰æ¡†

# ç­‰å¾…
await page.wait_for_selector("selector") # ç­‰å¾…å…ƒç´ 
await page.wait_for_timeout(2000)        # ç­‰å¾…æ—¶é—´
```

---

## ğŸ“ å®Œæ•´æ–‡æ¡£

- **è¯¦ç»†æ‰©å±•æŒ‡å—**: `docs/ADD_NEW_INSURANCE_COMPANY.md`
- **åŸºç±»ä»£ç **: `src/crawler/discovery/base_spider.py`
- **å·¥å‚ç±»ä»£ç **: `src/crawler/discovery/spider_factory.py`
- **ç¤ºä¾‹å®ç°**: `src/crawler/discovery/chinalifeinsurance_spider.py`

---

**æ€»ç»“**ï¼šæ‚¨å½“å‰çš„ `pingan_life_spider.py` æ˜¯ä¸€ä¸ªå®Œæ•´ã€å¯å·¥ä½œçš„çˆ¬è™«å®ç°ã€‚è¦æ·»åŠ å…¶ä»–ä¿é™©å…¬å¸ï¼Œå»ºè®®ä½¿ç”¨æˆ‘åˆ›å»ºçš„åŸºç±»æ¶æ„ï¼Œé€šè¿‡ç»§æ‰¿ `BaseInsuranceSpider` æ¥å¿«é€Ÿå®ç°æ–°å…¬å¸çš„çˆ¬è™«ï¼Œä»£ç æ›´æ¸…æ™°ã€æ›´æ˜“ç»´æŠ¤ã€‚

